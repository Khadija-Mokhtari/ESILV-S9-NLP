{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935f6e12",
   "metadata": {},
   "source": [
    "# TP1 : NLP, Khadija MOKHTARI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259cf5d6",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d9fe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Khadi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Khadi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Khadi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  # for tokenization\n",
    "nltk.download('stopwords')  # for stop words\n",
    "nltk.download('wordnet')  # for lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c4ce2",
   "metadata": {},
   "source": [
    "##### Tokenization of text into sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e369d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Hello everyone.',\n",
       " 'Hope all are fine and doing well.',\n",
       " 'Hope you find the book interesting']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text=\" Hello everyone. Hope all are fine and doing well. Hope you find the book interesting\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e63aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization of text in other languages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d27cea13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collège franco-britannique de Levallois-Perret.',\n",
       " 'Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois.',\n",
       " \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire.\",\n",
       " \"L'équipepédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'unprofesseur d'histoire\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "french_tokenizer=nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "french_tokenizer.tokenize('Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collège franco-britannique de Levallois-Perret. Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois. L\\'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l\\'agression, janvier , d\\'un professeur d\\'histoire. L\\'équipepédagogique de ce collège de 750 élèves avait déjà été choquée par l\\'agression, mercredi , d\\'unprofesseur d\\'histoire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a41f3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization of sentences into words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c63fdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'day.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'the',\n",
       " 'book',\n",
       " 'interesting']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"Have a nice day. I hope you find the book interesting\")\n",
    "\n",
    "## What we expect to have : ['Have', 'a', 'nice', 'day.', 'I', 'hope', 'you', 'find', 'the', 'book', 'interesting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a114c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text=nltk.word_tokenize(\" Don't hesitate to ask questions\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8848ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another word tokenizer is PunktWordTokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30a39958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer=WordPunctTokenizer()\n",
    "tokenizer.tokenize(\" Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e661376",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization using regular expressions(regex):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cdba438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', 't', 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer.tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb295e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'t\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "## Instead of instantiating class, an alternative way of tokenization would be to use this function:\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "sent=\"Don't hesitate to ask questions\"\n",
    "print(regexp_tokenize(sent, pattern='\\w+|\\$[\\d\\.]+|\\S+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b15d6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of instantiating class, an alternative way of tokenization would be to use this function:\n",
    "\n",
    "# import nltk\n",
    "# from nltk.tokenize import regexp_tokenize\n",
    "# sent=\"Don't hesitate to ask questions\"\n",
    "# print(regexp_tokenize(sent, pattern='\\w+|\\$[\\d\\.]+|\\S+'))\n",
    "\n",
    "## Same as previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153b7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d94146bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conversion into lowercase and uppercase :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f45f3a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardwork is key to success\n"
     ]
    }
   ],
   "source": [
    "text='HARdWork IS KEy to SUCCESS'\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd2db33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARDWORK IS KEY TO SUCCESS\n"
     ]
    }
   ],
   "source": [
    "print(text.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4905ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dealing with stop words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36b5d845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesitate', 'ask', 'questions']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words=[\"Don't\", 'hesitate','to','ask','questions']\n",
    "[word for word in words if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ca2c397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d5cc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of the replacement of a text with another text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e28f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "  (r'won\\'t', 'will not'),\n",
    "\t(r'can\\'t', 'cannot'),\n",
    "\t(r'i\\'m', 'i am'),\n",
    "\t(r'ain\\'t', 'is not'),\n",
    "\t(r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "\t(r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "\t(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "\t(r'(\\w+)\\'s', '\\g<1> is'),\n",
    "\t(r'(\\w+)\\'re', '\\g<1> are'),\n",
    "\t(r'(\\w+)\\'d', '\\g<1> would'),\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "   def __init__(self, patterns=replacement_patterns):\n",
    "      self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "\n",
    "   def replace(self, text):\n",
    "      s = text\n",
    "      for (pattern, repl) in self.patterns:\n",
    "           s = re.sub(pattern, repl, s)\n",
    "      return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9180102b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do not hesitate to ask questions'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "replacer= RegexpReplacer()\n",
    "replacer.replace(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d6a7af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Performing substitution before tokenization :\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "replacer=RegexpReplacer()\n",
    "word_tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ade283a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', 'not', 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(replacer.replace(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2df8a1",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbfefb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'working'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output=WordNetLemmatizer()\n",
    "lemmatizer_output.lemmatize('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0bfbc45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_output.lemmatize('working',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58fe6b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer_output.lemmatize('works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3884fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer_output=PorterStemmer()\n",
    "stemmer_output.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a53b5a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happiness'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output=WordNetLemmatizer()\n",
    "lemmatizer_output.lemmatize('happiness')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66af01",
   "metadata": {},
   "source": [
    "## Similarity measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ac5c983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import *\n",
    "edit_distance(\"relate\",\"relation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b3a420e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance(\"suggestion\",\"calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d6dd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying similarity measures using Jaccard's Coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd1e6756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.metrics import *\n",
    "X=set([10,20,30,40])\n",
    "Y=set([20,30,60])\n",
    "print(jaccard_distance(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7561ee8",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5431f59",
   "metadata": {},
   "source": [
    "## Practical work : two article about space and another one about the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ab2fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deux fois plus grande que la Terre.',\n",
       " 'Les exoplanètes sont des planètes situées hors du système solaire et orbitant autour d’une étoile.',\n",
       " 'Mais contrairement à la plupart d’entre elles, HD88986b possède une température relativement faible de 190\\u2009°C.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "french_tokenizer=nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "text = \"Deux fois plus grande que la Terre. Les exoplanètes sont des planètes situées hors du système solaire et orbitant autour d’une étoile. Mais contrairement à la plupart d’entre elles, HD88986b possède une température relativement faible de 190 °C.\"\n",
    "french_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f68b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)  # sentence tokenization\n",
    "words = word_tokenize(text)  # word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff948226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deux fois plus grande que la Terre.',\n",
       " 'Les exoplanètes sont des planètes situées hors du système solaire et orbitant autour d’une étoile.',\n",
       " 'Mais contrairement à la plupart d’entre elles, HD88986b possède une température relativement faible de 190\\u2009°C.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7c9673d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deux',\n",
       " 'fois',\n",
       " 'plus',\n",
       " 'grande',\n",
       " 'que',\n",
       " 'la',\n",
       " 'Terre',\n",
       " '.',\n",
       " 'Les',\n",
       " 'exoplanètes',\n",
       " 'sont',\n",
       " 'des',\n",
       " 'planètes',\n",
       " 'situées',\n",
       " 'hors',\n",
       " 'du',\n",
       " 'système',\n",
       " 'solaire',\n",
       " 'et',\n",
       " 'orbitant',\n",
       " 'autour',\n",
       " 'd',\n",
       " '’',\n",
       " 'une',\n",
       " 'étoile',\n",
       " '.',\n",
       " 'Mais',\n",
       " 'contrairement',\n",
       " 'à',\n",
       " 'la',\n",
       " 'plupart',\n",
       " 'd',\n",
       " '’',\n",
       " 'entre',\n",
       " 'elles',\n",
       " ',',\n",
       " 'HD88986b',\n",
       " 'possède',\n",
       " 'une',\n",
       " 'température',\n",
       " 'relativement',\n",
       " 'faible',\n",
       " 'de',\n",
       " '190',\n",
       " '°C',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0789f7",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7470f981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deux fois plus grande que la terre. les exoplanètes sont des planètes situées hors du système solaire et orbitant autour d’une étoile. mais contrairement à la plupart d’entre elles, hd88986b possède une température relativement faible de 190\\u2009°c.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lower = text.lower()\n",
    "text_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b77b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_filtered = [word for word in words if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac962fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deux',\n",
       " 'fois',\n",
       " 'plus',\n",
       " 'grande',\n",
       " 'que',\n",
       " 'la',\n",
       " 'Terre',\n",
       " '.',\n",
       " 'Les',\n",
       " 'exoplanètes',\n",
       " 'sont',\n",
       " 'des',\n",
       " 'planètes',\n",
       " 'situées',\n",
       " 'hors',\n",
       " 'du',\n",
       " 'système',\n",
       " 'solaire',\n",
       " 'et',\n",
       " 'orbitant',\n",
       " 'autour',\n",
       " '’',\n",
       " 'une',\n",
       " 'étoile',\n",
       " '.',\n",
       " 'Mais',\n",
       " 'contrairement',\n",
       " 'à',\n",
       " 'la',\n",
       " 'plupart',\n",
       " '’',\n",
       " 'entre',\n",
       " 'elles',\n",
       " ',',\n",
       " 'HD88986b',\n",
       " 'possède',\n",
       " 'une',\n",
       " 'température',\n",
       " 'relativement',\n",
       " 'faible',\n",
       " 'de',\n",
       " '190',\n",
       " '°C',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c046e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "282226fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deux',\n",
       " 'fois',\n",
       " 'plus',\n",
       " 'grande',\n",
       " 'que',\n",
       " 'la',\n",
       " 'Terre',\n",
       " '.',\n",
       " 'Les',\n",
       " 'exoplanètes',\n",
       " 'sont',\n",
       " 'de',\n",
       " 'planètes',\n",
       " 'situées',\n",
       " 'hors',\n",
       " 'du',\n",
       " 'système',\n",
       " 'solaire',\n",
       " 'et',\n",
       " 'orbitant',\n",
       " 'autour',\n",
       " '’',\n",
       " 'une',\n",
       " 'étoile',\n",
       " '.',\n",
       " 'Mais',\n",
       " 'contrairement',\n",
       " 'à',\n",
       " 'la',\n",
       " 'plupart',\n",
       " '’',\n",
       " 'entre',\n",
       " 'elles',\n",
       " ',',\n",
       " 'HD88986b',\n",
       " 'possède',\n",
       " 'une',\n",
       " 'température',\n",
       " 'relativement',\n",
       " 'faible',\n",
       " 'de',\n",
       " '190',\n",
       " '°C',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cad082",
   "metadata": {},
   "source": [
    "### Jaccard distance score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b6d3e",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9aece99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import edit_distance, jaccard_distance\n",
    "\n",
    "# Example of using edit distance and Jaccard's distance\n",
    "distance_edit = edit_distance(\"word1\", \"word2\")\n",
    "set1, set2 = set(['a', 'b', 'c']), set(['b', 'c', 'd'])\n",
    "distance_jaccard = jaccard_distance(set1, set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b04435b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa169d",
   "metadata": {},
   "source": [
    "### Between two sentences of the two articles that have the same topic, the space topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4e212dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sentences from each text file about space\n",
    "sentence1 = \"Bien qu’elle soit considérée comme un astre de petite taille, elle est deux fois plus grande et dix-sept fois plus lourde que la Terre.\"\n",
    "sentence2 = \"L’appareil se trouve actuellement sur une trajectoire entre la Terre et la Lune et Astrobotic avait indiqué tout mettre en œuvre pour allonger sa durée de vie au maximum et permettre à ses instruments embarqués de collecter et transmettre des données.\"\n",
    "\n",
    "# Tokenization and normalization (convert to lowercase)\n",
    "words1 = word_tokenize(sentence1.lower())\n",
    "words2 = word_tokenize(sentence2.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8ee711dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words1 = [word for word in words1 if word not in stop_words]\n",
    "filtered_words2 = [word for word in words2 if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "67989db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jaccard distance between the sentences is: 0.896551724137931\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import jaccard_distance\n",
    "\n",
    "set1 = set(filtered_words1)\n",
    "set2 = set(filtered_words2)\n",
    "\n",
    "# Calculate Jaccard distance\n",
    "distance = jaccard_distance(set1, set2)\n",
    "print(f\"The Jaccard distance between the sentences is: {distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f8c38",
   "metadata": {},
   "source": [
    "It actually works pretty well !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
